{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "from prettytable import PrettyTable\n",
    "from scipy import sparse\n",
    "import csv\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score, cross_validate\n",
    "from sklearn.preprocessing import StandardScaler, Normalizer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_curve, auc\n",
    "from sklearn.metrics import confusion_matrix, classification_report, precision_score\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
    "\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150000, 10)\n",
      "Wall time: 16.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "con = sqlite3.connect('database.sqlite')\n",
    "raw_data = pd.read_sql_query(\"\"\" SELECT * FROM Reviews WHERE Score != 3 \"\"\", con)\n",
    "raw_data = raw_data.sample(150000, random_state=42)\n",
    "raw_data = raw_data.sort_values('Time')\n",
    "print(raw_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data):\n",
    "    '''\n",
    "    Preprocess I/P data :-\n",
    "    1. Update Score with 0 (-ve Review) and 1 (+ve Review).\n",
    "    2. Add another feature with Review length and use Log to reduce scale.\n",
    "    3. Select unique rows based on - UserId, ProfileName, Time and Text.\n",
    "    4. Remove rows which have helpfulness denominator greater than helpfulness numerator.\n",
    "    5. Remove html tags from Reviews.\n",
    "    6. Remove punctuations and special characters.\n",
    "    7. Performing Stemming using Snowball Stemmer.\n",
    "    8. Extract Adjectives, Nouns, Verbs, Adverbs from 'Summary' column and add it to the 'CleanedText' column.\n",
    "    '''\n",
    "    # Update 'Score' with 0 -> Negative Review and 1 -> Positive Review\n",
    "    data['Score'] = data['Score'].map(lambda x: 0 if x<3 else 1)\n",
    "    data['TextLength'] = data['Text'].apply(lambda x: math.log(len(x.split())))\n",
    "\n",
    "    data = data.drop_duplicates(subset={'UserId', 'ProfileName', 'Time', 'Text'})\n",
    "    data = data[data['HelpfulnessNumerator'] <= data['HelpfulnessDenominator']]\n",
    "\n",
    "    stops = set(stopwords.words('english')) - set(['not'])\n",
    "    snow = SnowballStemmer('english')\n",
    "\n",
    "    # Function to clean the word of any html-tags\n",
    "    def cleanhtml(sentence):\n",
    "        cleanr = re.compile('<.*?>')\n",
    "        cleantext = re.sub(cleanr, ' ', sentence)\n",
    "        return cleantext\n",
    "\n",
    "    # Function to clean the word of any punctuation or special characters\n",
    "    def cleanpunc(sentence):\n",
    "        cleaned = re.sub(r'[?|!|\\'|\"|:|#]',r' ',sentence)\n",
    "        cleaned = re.sub(r'[.|,|)|(|\\|/]',r' ',cleaned)\n",
    "        return  cleaned\n",
    "\n",
    "    filtered_reviews = []\n",
    "    s = ''\n",
    "\n",
    "    for review in data['Text'].values:\n",
    "        filtered_sent = []\n",
    "        review = cleanhtml(review)\n",
    "        review = cleanpunc(review)\n",
    "        for word in review.split():\n",
    "            if word.isalpha() and len(word) > 2:\n",
    "                if word.lower() not in stops:\n",
    "                    s = snow.stem(word.lower())\n",
    "                    filtered_sent.append(s)\n",
    "        filtered_reviews.append(' '.join(filtered_sent))\n",
    "        \n",
    "    tags = []\n",
    "    for review in data['Summary'].values:\n",
    "        review = cleanhtml(review)\n",
    "        review = cleanpunc(review)\n",
    "        adjs = [x[0] for x in nltk.pos_tag(word_tokenize(review)) if x[1].startswith('JJ') \\\n",
    "                or x[1].startswith('RB') or x[1].startswith('VB') or x[1] == 'NNP']\n",
    "        if len(adjs) > 0:\n",
    "            adjs = ' '.join([snow.stem(word.lower()) if len(word) > 2 and word.lower() not in stops else '' for word in adjs])\n",
    "        tags.append(adjs)\n",
    "    \n",
    "    print('Total Data-points :-',len(filtered_reviews))\n",
    "    data['CleanedText'] = [str(filtered_reviews[i]) + ' ' + str(tags[i]) for i in range(len(tags))]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(data_points):\n",
    "    '''\n",
    "    Perform pre-processing on raw data and extract required datapoints :: 70-30 Split.\n",
    "    '''\n",
    "    split = int(data_points * 0.70)\n",
    "    data = preprocess(raw_data)\n",
    "    train = data[:split]\n",
    "    test = data[split:data_points]\n",
    "\n",
    "    print(train['Score'].value_counts())\n",
    "    print(test['Score'].value_counts())\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Data-points :- 124264\n",
      "1    12473\n",
      "0     1527\n",
      "Name: Score, dtype: int64\n",
      "1    5250\n",
      "0     750\n",
      "Name: Score, dtype: int64\n",
      "Wall time: 3min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data_train, data_test = get_data(20000)\n",
    "del raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 590 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data_train.to_csv('./Matrices/sample_data_train.csv', index=False)\n",
    "data_test.to_csv('./Matrices/sample_data_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Data-points :- 124264\n",
      "1    24674\n",
      "0     3326\n",
      "Name: Score, dtype: int64\n",
      "1    10304\n",
      "0     1696\n",
      "Name: Score, dtype: int64\n",
      "Wall time: 6min 45s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data_train, data_test = get_data(40000)\n",
    "del raw_data\n",
    "\n",
    "data_train.to_csv('./Matrices/sample2_data_train.csv', index=False)\n",
    "data_test.to_csv('./Matrices/sample2_data_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14000, 4968) (6000, 4968)\n",
      "Wall time: 1.48 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cv_model = CountVectorizer(max_features=15000, min_df=5)\n",
    "bow_counts_train = cv_model.fit_transform(data_train['CleanedText'].values)\n",
    "bow_counts_test = cv_model.transform(data_test['CleanedText'].values)\n",
    "print(bow_counts_train.shape, bow_counts_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse.save_npz('./Matrices/sample_bow_train.npz', bow_counts_train)\n",
    "sparse.save_npz('./Matrices/sample_bow_test.npz', bow_counts_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Data-points :- 124264\n",
      "1    48338\n",
      "0     7662\n",
      "Name: Score, dtype: int64\n",
      "1    19901\n",
      "0     4099\n",
      "Name: Score, dtype: int64\n",
      "Wall time: 3min 28s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data_train, data_test = get_data(80000)\n",
    "del raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4.36 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data_train.to_csv('./Matrices/data_train.csv', index=False)\n",
    "data_test.to_csv('./Matrices/data_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(56000, 9397) (24000, 9397)\n",
      "Wall time: 3.27 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cv_model = CountVectorizer(max_features=15000, min_df=5)\n",
    "bow_counts_train = cv_model.fit_transform(data_train['CleanedText'].values)\n",
    "bow_counts_test = cv_model.transform(data_test['CleanedText'].values)\n",
    "print(bow_counts_train.shape, bow_counts_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse.save_npz('./Matrices/bow_train.npz', bow_counts_train)\n",
    "sparse.save_npz('./Matrices/bow_test.npz', bow_counts_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(56000, 9397) (24000, 9397)\n",
      "Wall time: 3.34 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tfidf_model = TfidfVectorizer(max_features=15000, min_df=5)\n",
    "tfidf_train = tfidf_model.fit_transform(data_train['CleanedText'].values)\n",
    "tfidf_test = tfidf_model.transform(data_test['CleanedText'].values)\n",
    "print(tfidf_train.shape, tfidf_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse.save_npz('./Matrices/tfidf_train.npz', tfidf_train)\n",
    "sparse.save_npz('./Matrices/tfidf_test.npz', tfidf_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56000 24000\n",
      "Wall time: 3min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def avg_w2v(data_train, data_test):\n",
    "    # Process Train Data\n",
    "    train_list_of_sent=[]\n",
    "    for sent in data_train['CleanedText'].values:\n",
    "        train_list_of_sent.append(sent.split())\n",
    "\n",
    "    # Build W2V model based on Train Data only\n",
    "    w2v_model = Word2Vec(train_list_of_sent, min_count=5, size=50, workers=4)\n",
    "    w2v_words = list(w2v_model.wv.vocab)\n",
    "\n",
    "    sent_vectors_train = []\n",
    "    for sent in train_list_of_sent:\n",
    "        sent_vec = np.zeros(50)\n",
    "        count_words = 0\n",
    "        for word in sent:\n",
    "            if word in w2v_words:\n",
    "                try:\n",
    "                    vec = w2v_model.wv[word]\n",
    "                except KeyError:\n",
    "                    vec = np.zeros(50)\n",
    "                sent_vec += vec\n",
    "                count_words += 1\n",
    "        if count_words != 0:\n",
    "            sent_vec /= count_words\n",
    "        sent_vectors_train.append(sent_vec)\n",
    "\n",
    "    # Process Test Data\n",
    "    test_list_of_sent=[]\n",
    "    for sent in data_test['CleanedText'].values:\n",
    "        test_list_of_sent.append(sent.split())\n",
    "\n",
    "    sent_vectors_test = []\n",
    "    for sent in test_list_of_sent:\n",
    "        sent_vec = np.zeros(50)\n",
    "        count_words = 0\n",
    "        for word in sent:\n",
    "            if word in w2v_words:\n",
    "                try:\n",
    "                    vec = w2v_model.wv[word]      # Use W2V model based on Train data to create the text Vectors.\n",
    "                except KeyError:\n",
    "                    vec = np.zeros(50)\n",
    "                sent_vec += vec\n",
    "                count_words += 1\n",
    "        if count_words != 0:\n",
    "            sent_vec /= count_words\n",
    "        sent_vectors_test.append(sent_vec)\n",
    "    \n",
    "    return sent_vectors_train, sent_vectors_test\n",
    "\n",
    "sent_vectors_train, sent_vectors_test = avg_w2v(data_train, data_test)\n",
    "print(len(sent_vectors_train), len(sent_vectors_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(sent_vectors_train).to_csv('./Matrices/avg_w2v_train.csv', index=False)\n",
    "pd.DataFrame(sent_vectors_test).to_csv('./Matrices/avg_w2v_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56000 24000\n",
      "Wall time: 3min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def tfidf_w2v(data_train, data_test):\n",
    "    tfidf = TfidfVectorizer()\n",
    "    tfidf_vect_train = tfidf.fit_transform(data_train['CleanedText'].values).toarray()\n",
    "    tfidf_vect_test = tfidf.transform(data_test['CleanedText'].values).toarray()\n",
    "    words_dict = dict(zip(tfidf.get_feature_names(), list(tfidf.idf_)))\n",
    "\n",
    "    # Process Train Data\n",
    "    list_of_sent=[]\n",
    "    for sent in data_train['CleanedText'].values:\n",
    "        list_of_sent.append(sent.split())\n",
    "    \n",
    "    # Build W2V model based on Train Data only\n",
    "    w2v_model = Word2Vec(list_of_sent, min_count=5, size=50, workers=4)\n",
    "    w2v_words = list(w2v_model.wv.vocab)\n",
    "\n",
    "    tfidf_sent_vectors_train = []\n",
    "    row=0\n",
    "    for sent in list_of_sent:\n",
    "        sent_vec = np.zeros(50)\n",
    "        weighted_sum = 0\n",
    "        for word in sent:\n",
    "            if word in w2v_words:\n",
    "                try:\n",
    "                    vec = w2v_model.wv[word]\n",
    "                except KeyError:\n",
    "                    vec = np.ones(50)\n",
    "                try:\n",
    "                    tf_idf = words_dict[word]*(sent.count(word)/len(sent))\n",
    "                except KeyError:\n",
    "                    tf_idf = 1.0*(sent.count(word)/len(sent))\n",
    "                sent_vec += (vec * tf_idf)\n",
    "                weighted_sum += tf_idf\n",
    "        if weighted_sum != 0:\n",
    "            sent_vec /= weighted_sum\n",
    "        tfidf_sent_vectors_train.append(sent_vec)\n",
    "        row += 1\n",
    "\n",
    "    # Process Test Data\n",
    "    list_of_sent=[]\n",
    "    for sent in data_test['CleanedText'].values:\n",
    "        list_of_sent.append(sent.split())\n",
    "\n",
    "    tfidf_sent_vectors_test = []\n",
    "    row=0\n",
    "    for sent in list_of_sent:\n",
    "        sent_vec = np.zeros(50)\n",
    "        weighted_sum = 0\n",
    "        for word in sent:\n",
    "            if word in w2v_words:\n",
    "                try:\n",
    "                    vec = w2v_model.wv[word]       # Use W2V model based on Train data to create the text Vectors.\n",
    "                except KeyError:\n",
    "                    vec = np.ones(50)\n",
    "                try:\n",
    "                    tf_idf = words_dict[word]*(sent.count(word)/len(sent))\n",
    "                except KeyError:\n",
    "                    tf_idf = 1.0*(sent.count(word)/len(sent))\n",
    "                sent_vec += (vec * tf_idf)\n",
    "                weighted_sum += tf_idf\n",
    "        if weighted_sum != 0:\n",
    "            sent_vec /= weighted_sum\n",
    "        tfidf_sent_vectors_test.append(sent_vec)\n",
    "        row += 1\n",
    "    \n",
    "    return tfidf_sent_vectors_train, tfidf_sent_vectors_test\n",
    "\n",
    "tfidf_sent_vectors_train, tfidf_sent_vectors_test = tfidf_w2v(data_train, data_test)\n",
    "print(len(tfidf_sent_vectors_train), len(tfidf_sent_vectors_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(tfidf_sent_vectors_train).to_csv('./Matrices/tfidf_w2v_train.csv', index=False)\n",
    "pd.DataFrame(tfidf_sent_vectors_test).to_csv('./Matrices/tfidf_w2v_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
